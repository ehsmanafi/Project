{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforcementLearningWhaleOptimizationAlgorithm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "wFK5-UoF0K0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we develop an Intelligent Whale Optimization (IWO) algorithm to solve dual-resourced flexible job-shop scheduling problem. To do this, we train an agent which adjusts the key parameters of the Whale Optimization (WO) algorithm. This helps to improve the global search ability of WO."
      ],
      "metadata": {
        "id": "uRXzcdpTzJyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the libraries"
      ],
      "metadata": {
        "id": "KQBxXZ6b0DSw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMk9zjtHxqea"
      },
      "outputs": [],
      "source": [
        "!pip install 'tensorflow==1.15.2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gym==0.19.0\""
      ],
      "metadata": {
        "id": "mfoV1o8Zyim4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n",
        "!pip install \"stable-baselines[mpi]==2.2.1\""
      ],
      "metadata": {
        "id": "zeQGzg9Hyphu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem instances"
      ],
      "metadata": {
        "id": "tC6D9gcN2fdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box, MultiDiscrete\n",
        "import random\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import SubprocVecEnv,VecEnv\n",
        "from stable_baselines.common import set_global_seeds\n",
        "from stable_baselines import ACKTR,A2C\n",
        "\n",
        "def problem_data(Seed):\n",
        "  np.random.seed(Seed)\n",
        "  Machine=[i for i in range(1,np.random.choice([5,6,7,8,9,10]))] \n",
        "  Configuration={i:np.random.choice([2,3,4,5]) for i in Machine} \n",
        "  Job={i:np.random.choice([2,3,4,5]) for i in range(1,np.random.choice([3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]))} \n",
        "  W=[i for i in range(1,np.random.choice([3,4,5,6,7,8,9,10,11]))] \n",
        "  R={(ii,jj):{(i,j,w):np.random.choice([2,3,4,5,6,7,8,9]) for w in W for i in Configuration.keys() for j in range(1,Configuration[i]+1)} for ii in Job.keys() for jj in range(1,Job[ii]+1)}  \n",
        "  RecChange={ii:{(i,j):np.random.choice([2,3,4,5,6]) for i in range(1,1+Configuration[ii]) for j in range(1,1+Configuration[ii])} for ii in Configuration.keys()} \n",
        "  return Machine,Configuration,Job,R,RecChange,W"
      ],
      "metadata": {
        "id": "Y_uPfSCVy0Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "t_L7A8pS2pR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WOAEnv(Env):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.Seed=0\n",
        "    self.Machine,self.Configuration,self.Job,self.R,self.RecChange,self.W=problem_data(self.Seed)\n",
        "    self.action_space = MultiDiscrete([10,5])\n",
        "    #Box(low=np.array([0,0],dtype=np.float32), high=np.array([2,1],dtype=np.float32),dtype=np.float32)\n",
        "    self.observation_space = Box(low=np.array([0,0,0,0,0,0,0,0,0],dtype=np.float32), high=np.array([1,1,1,1,1,1,1,1,1],dtype=np.float32),dtype=np.float32)\n",
        "    self.bestpop=[]\n",
        "    self.MaxTime = 200\n",
        "    self.time=0\n",
        "    self.tt=0\n",
        "    self.costo=[]\n",
        "    self.AVF1=0\n",
        "    self.AVF=1\n",
        "    self.Div1=0\n",
        "    self.Div=1\n",
        "    self.bestWOA=1\n",
        "    self.bestt1=0\n",
        "    self.bestt=1\n",
        "    self.rew=0\n",
        "    self.fitisum=0\n",
        "    self.AV5=0\n",
        "    self.AV10=0\n",
        "    self.AV25=0\n",
        "    self.AV50=0\n",
        "    self.A=1.0\n",
        "    self.prob=0.5\n",
        "    self.mean=0\n",
        "    self.std=0\n",
        "    #self.state = np.array((self.prob,self.A,self.time,self.AV5,self.AV10,self.AV25,self.AV50),dtype=np.float32)\n",
        "    self.state = np.array((self.tt,self.mean,self.AV5,self.AV10,self.AV25,self.AV50,self.AVF,self.Div,self.bestt),dtype=np.float32)\n",
        "    self.dim=200\n",
        "    self.SearchAgents_no=30\n",
        "    self.Max_iter=self.MaxTime\n",
        "    self.Leader_pos=np.zeros([1,self.dim]).tolist()\n",
        "    self.Leader_score=float('inf')\n",
        "    self.Positions=np.random.random([self.SearchAgents_no,self.dim]).tolist()\n",
        "    self.Convergence_curve=np.zeros([1,self.Max_iter+2]).tolist()[0]\n",
        "    \n",
        "\n",
        "      \n",
        "  def step(self, action):\n",
        "    self.prob=action[1]/5.01\n",
        "    self.A=action[0]/5.01\n",
        "    lb=0.001\n",
        "    ub=0.999\n",
        "    dim=200\n",
        "    SearchAgents_no=30\n",
        "    Best_score,Best_pos,WOA_cg_curve=self.WOA(SearchAgents_no,self.MaxTime,lb,ub,dim,self.CMAX)\n",
        "    if self.time ==0: \n",
        "        reward =0\n",
        "    else: \n",
        "        reward =self.rew+3*((self.bestWOA-Best_score)/self.bestWOA)\n",
        "    self.bestWOA=Best_score\n",
        "    self.bestpop.append(self.bestWOA)\n",
        "    self.mean=self.bestWOA/np.mean(self.costo)\n",
        "    #self.std=(np.mean(self.costo)-self.bestWOA)/np.std(self.costo)\n",
        "    self.AV5=-(self.bestWOA-np.mean(self.bestpop[-5:]))/np.mean(self.bestpop[-5:])\n",
        "    self.AV10=-(self.bestWOA-np.mean(self.bestpop[-10:]))/np.mean(self.bestpop[-10:])\n",
        "    self.AV25=-(self.bestWOA-np.mean(self.bestpop[-25:]))/np.mean(self.bestpop[-25:])\n",
        "    self.AV50=-(self.bestWOA-np.mean(self.bestpop[-50:]))/np.mean(self.bestpop[-50:])\n",
        "    self.time+=1\n",
        "    self.tt=self.time/self.MaxTime\n",
        "    if self.time >= self.MaxTime: \n",
        "        done = True\n",
        "    else:\n",
        "        done = False\n",
        "    info = {}\n",
        "    self.costo=[]\n",
        "    return np.array((self.tt,self.mean,self.AV5,self.AV10,self.AV25,self.AV50,self.AVF,self.Div,self.bestt),dtype=np.float32), reward, done, info\n",
        "\n",
        "  def render(self):\n",
        "      # Implement viz\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    # Reset shower temperature\n",
        "    #self.Machine,self.Configuration,self.Job,self.AMRs,self.R,self.Nodes,self.Nodesdistance,self.RecChange=problem_data()\n",
        "    self.Seed+=1\n",
        "    self.Machine,self.Configuration,self.Job,self.R,self.RecChange,self.W=problem_data(self.Seed)\n",
        "    self.p=0.5\n",
        "    # Reset shower time\n",
        "    self.A=1\n",
        "    self.bestWOA=1\n",
        "    self.MaxTime=200\n",
        "    self.bestpop=[]\n",
        "    self.AVF1=0\n",
        "    self.AVF=1\n",
        "    self.Div1=0\n",
        "    self.Div=1\n",
        "    self.bestt1=0\n",
        "    self.bestt=1\n",
        "    self.rew=0\n",
        "    self.fitisum=0\n",
        "    self.AV5=0\n",
        "    self.AV10=0\n",
        "    self.AV25=0\n",
        "    self.AV50=0\n",
        "    self.costo=[]\n",
        "    self.mean=0\n",
        "    self.std=0\n",
        "    self.time=0\n",
        "    self.tt=0\n",
        "    self.Leader_pos=np.zeros([1,self.dim]).tolist()\n",
        "    self.Leader_score=float('inf')\n",
        "    self.Positions=np.random.random([self.SearchAgents_no,self.dim]).tolist()\n",
        "    self.Convergence_curve=np.zeros([1,self.Max_iter+2]).tolist()[0]\n",
        "    print('Reset')\n",
        "    return np.array((self.tt,self.mean,self.AV5,self.AV10,self.AV25,self.AV50,self.AVF,self.Div,self.bestt),dtype=np.float32)#np.array((self.p,self.b),dtype=np.float32)#np.array((self.temprature,self.shower_length),dtype=np.float32)\n",
        "    \n",
        "  def WOA(self,SearchAgents_no,Max_iter,lb,ub,dim,fobj):\n",
        "    t=self.time\n",
        "    if t==0:\n",
        "      for i in range(0,len(self.Positions)):\n",
        "          Flag4ub=(np.array(self.Positions[i])>np.array([ub,]*dim))\n",
        "          Flag4lb=(np.array(self.Positions[i])<np.array([lb,]*dim))\n",
        "          self.Positions[i][:]=((np.array(self.Positions[i][:])*(~(Flag4ub|Flag4lb)))+0.9*Flag4ub+0.1*Flag4lb).tolist()\n",
        "          fitness=fobj(self.Positions[i])\n",
        "          if fitness<self.Leader_score:\n",
        "              self.Leader_score=fitness\n",
        "              self.Leader_pos=self.Positions[i][:]\n",
        "    a=2-t*((2)/Max_iter)\n",
        "    a2=-1+t*((-1)/Max_iter)\n",
        "    for i in range(0,len(self.Positions)):\n",
        "        r1=np.random.rand()\n",
        "        r2=np.random.rand()\n",
        "        A=self.A#2*a*r1-a\n",
        "        C=2*r2\n",
        "        b=1\n",
        "        l=(a2-1)*np.random.rand()+1\n",
        "        p = np.random.rand()\n",
        "      \n",
        "        for j in range(len(self.Positions[1])):\n",
        "            if p<self.prob:   \n",
        "                if abs(A)>=1:\n",
        "                    rand_leader_index = int(np.floor(SearchAgents_no*np.random.rand()))\n",
        "                    X_rand = self.Positions[rand_leader_index][:]\n",
        "                    D_X_rand=abs(C*X_rand[j]-self.Positions[i][j])\n",
        "                    self.Positions[i][j]=X_rand[j]-A*D_X_rand\n",
        "                elif abs(A)<1:\n",
        "                    D_Leader=abs(C*self.Leader_pos[j]-self.Positions[i][j])\n",
        "                    self.Positions[i][j]=self.Leader_pos[j]-A*D_Leader\n",
        "            elif p>=self.prob:\n",
        "                distance2Leader=abs(self.Leader_pos[j]-self.Positions[i][j])\n",
        "                self.Positions[i][j]=distance2Leader*np.exp(b*l)*np.cos(l*2*np.pi)+self.Leader_pos[j]########??\n",
        "    #print('Self.time:      ',self.time)    \n",
        "    self.Convergence_curve[self.time]=self.Leader_score\n",
        "    #print('Iteration:        ',t, '       Leader_score:     ',Leader_score,'        Leader_pos:     ',Leader_pos)\n",
        "    fitisum=0\n",
        "    for i in range(0,len(self.Positions)):\n",
        "      Flag4ub=(np.array(self.Positions[i])>np.array([ub,]*dim))\n",
        "      Flag4lb=(np.array(self.Positions[i])<np.array([lb,]*dim))\n",
        "      self.Positions[i][:]=((np.array(self.Positions[i][:])*(~(Flag4ub|Flag4lb)))+0.9*Flag4ub+0.1*Flag4lb).tolist()\n",
        "      fitness=fobj(self.Positions[i])\n",
        "      self.costo.append(fitness)\n",
        "      fitisum+=fitness\n",
        "      if fitness<self.Leader_score:\n",
        "          self.Leader_score=fitness\n",
        "          self.Leader_pos=self.Positions[i][:]\n",
        "    if t>0:\n",
        "        self.rew=(self.fitisum-fitisum)/self.fitisum\n",
        "    self.fitisum=fitisum\n",
        "    if t==0:\n",
        "      self.AVF1=fitisum\n",
        "      self.bestt1=self.Leader_score\n",
        "      Avsum=fitisum/self.SearchAgents_no\n",
        "      self.Div1=sum([abs(fobj(pp)-Avsum) for pp in self.Positions])\n",
        "    else:\n",
        "      self.AVF=fitisum/self.AVF1\n",
        "      self.bestt=self.Leader_score/self.bestt1\n",
        "      Avsum=fitisum/self.SearchAgents_no\n",
        "      self.Div=sum([abs(fobj(pp)-Avsum) for pp in self.Positions])/self.Div1\n",
        "    return self.Leader_score,self.Leader_pos,self.Convergence_curve\n",
        "\n",
        "  def CMAX(self, solution):\n",
        "    Machine=self.Machine\n",
        "    Configuration=self.Configuration\n",
        "    Job=self.Job\n",
        "    R=self.R\n",
        "    RecChange=self.RecChange\n",
        "    W=self.W\n",
        "    Rp={i:{} for i in R.keys()}\n",
        "    Scheduling=solution[:sum(list(Job.values()))]\n",
        "    MachineConfiguration=solution[sum(list(Job.values())):2*sum(list(Job.values()))]\n",
        "    AGV=solution[2*sum(list(Job.values())):3*sum(list(Job.values()))]\n",
        "    for i in Job.keys():\n",
        "      for j in range(1,Job[i]+1):\n",
        "        listt=[1/k for k in R[i,j].values()]\n",
        "        listtt=[k/(sum(listt)) for k in listt]\n",
        "        list4=[(listtt[k]+sum(listtt[:k])) for k in range(len(listtt))]\n",
        "        kkk=0\n",
        "        for kk in R[i,j].keys():\n",
        "          Rp[i,j][kk]=list4[kkk]\n",
        "          kkk+=1\n",
        "    Jobp=Job.copy()\n",
        "    Sequence={i:list() for i in range(1,sum(list(Job.values()))+1)}\n",
        "    op=lambda i : Job[i]-Jobp[i]+1\n",
        "\n",
        "    def ls(i,B):\n",
        "        return {\n",
        "            1: lambda: sum(Job[j] for j in range(1,i))+Job[i]-Jobp[i],\n",
        "            0: lambda: sum(Job[j] for j in range(1,i)),\n",
        "        }.get(B, lambda: None)()\n",
        "    \n",
        "    for index in Sequence.keys():\n",
        "      js={i:Scheduling[ls(i,(Jobp[i]>0)*1)] for i in Job.keys() }#if Jobp[i]>0}\n",
        "      selectedjob=np.where(min(js.values())==np.array(list(js.values())))[0][0]+1\n",
        "      a=MachineConfiguration[ls(selectedjob,(Jobp[selectedjob]>0)*1)]\n",
        "      mc=np.where(np.array(list(Rp[selectedjob,op(selectedjob)].values()))>=a)[0][0]\n",
        "      Sequence[index].append((selectedjob,op(selectedjob),list(R[selectedjob,op(selectedjob)].keys())[mc]))\n",
        "      Scheduling[ls(selectedjob,(Jobp[1]>0)*1)]=1\n",
        "      Jobp[selectedjob]-=1\n",
        "    #print('Sequence:  ',Sequence)\n",
        "\n",
        "    CJob={i:0 for i in Job.keys()}\n",
        "    CMachine={i:0 for i in Machine}\n",
        "    CWorker={i:0 for i in W}\n",
        "    MachineConf={i:1 for i in Machine}\n",
        "    for i in Sequence.keys():\n",
        "      jobb=Sequence[i][0][0]\n",
        "      operationn=Sequence[i][0][1]\n",
        "      machinee=Sequence[i][0][2][0]\n",
        "      configurationn=Sequence[i][0][2][1]\n",
        "      workerr=Sequence[i][0][2][2]\n",
        "      CMachine[machinee]+=RecChange[machinee][(MachineConf[machinee],configurationn)]\n",
        "      CMachine[machinee]=max([CMachine[machinee],CJob[jobb],CWorker[workerr]])+R[(jobb,operationn)][(machinee,configurationn,workerr)]\n",
        "      CJob[jobb]=CMachine[machinee]\n",
        "      obje=max(list(CJob.values()))\n",
        "    return obje"
      ],
      "metadata": {
        "id": "eUQUQz5FzIiz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training an agent, using actor-critic algorithm"
      ],
      "metadata": {
        "id": "FjZa0y_b28OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_envv(rank, seed=0):\n",
        "    def _init():\n",
        "        env = WOAEnv()\n",
        "        env.seed(seed + rank)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    num_cpu = 1\n",
        "    env = SubprocVecEnv([make_envv(i) for i in range(num_cpu)])\n",
        "    model = A2C(MlpPolicy, env, verbose=1)\n",
        "    model.learn(total_timesteps=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTOgwO7IzK6h",
        "outputId": "6faa6698-101f-4ad4-c83d-237c4a58da66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reset\n",
            "---------------------------------\n",
            "| explained_variance | 0.678    |\n",
            "| fps                | 0        |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 3.91     |\n",
            "| total_timesteps    | 5        |\n",
            "| value_loss         | 0.0252   |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Qpq8Itaz_eN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}